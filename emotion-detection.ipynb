{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "714df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFile\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e632c09",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our data will is in the DATASET folder split into our TRAIN and TEST datasets. Each folder contains one folder for each of our labels. Our next step will be to preprocess our data.\n",
    "\n",
    "- We use ImageDataGenerator to do some transformations of the images\n",
    "- Then we can apply the transformations to the directories for our train and test folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20762ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"DATASET/TRAIN\"\n",
    "test_dir = \"DATASET/TEST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0281467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1.0 / 255,\n",
    "    validation_split=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, preprocessing_function=preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a371cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=16,\n",
    "    subset=\"training\",\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=16,\n",
    "    subset=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c058182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b1ec26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_generator\u001b[39m.\u001b[39;49mclasses\u001b[39m.\u001b[39;49mcount(\u001b[39m6\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "train_generator.classes.count(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57a4020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3196, 1: 349, 2: 3278, 3: 5772, 4: 3972, 5: 3864, 6: 2537}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_generator.classes, return_counts=True)\n",
    "classes_count = dict(zip(unique, counts))\n",
    "print(classes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da35ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.10919899874843554, 1: 1.0, 2: 0.1064673581452105, 3: 0.060464310464310465, 4: 0.087865055387714, 5: 0.09032091097308488, 6: 0.13756405202995664}\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of samples in the smallest class\n",
    "num_samples_per_class = min(classes_count.values())\n",
    "\n",
    "# Use class weights to balance the dataset during training\n",
    "class_weights = {\n",
    "    c: num_samples_per_class / classes_count[c]\n",
    "    for c in range(train_generator.num_classes)\n",
    "}\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122c213c",
   "metadata": {},
   "source": [
    "We will use the VGG19 model you can read more about the requirements and considerations for this model in the documentation (https://keras.io/api/applications/vgg/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75ae9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG19(include_top=False, weights=\"imagenet\", input_shape=(48, 48, 3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b247471",
   "metadata": {},
   "source": [
    "### Adding flattening and dense layers\n",
    "\n",
    "Right now, our model is missing a top to actually classify our features. Let's add them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "433463e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,027,975\n",
      "Trainable params: 3,591\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17de85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1436/1436 [==============================] - 215s 149ms/step - loss: 0.2074 - accuracy: 0.1820 - val_loss: 1.8937 - val_accuracy: 0.2177\n",
      "Epoch 2/5\n",
      "1436/1436 [==============================] - 116s 81ms/step - loss: 0.1961 - accuracy: 0.2465 - val_loss: 1.8314 - val_accuracy: 0.2693\n",
      "Epoch 3/5\n",
      "1436/1436 [==============================] - 119s 83ms/step - loss: 0.1917 - accuracy: 0.2734 - val_loss: 1.8224 - val_accuracy: 0.2716\n",
      "Epoch 4/5\n",
      "1436/1436 [==============================] - 123s 85ms/step - loss: 0.1893 - accuracy: 0.2865 - val_loss: 1.7994 - val_accuracy: 0.2883\n",
      "Epoch 5/5\n",
      "1436/1436 [==============================] - 122s 85ms/step - loss: 0.1879 - accuracy: 0.2926 - val_loss: 1.8165 - val_accuracy: 0.2669\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss.\n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "new_model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history = new_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b03ca76f",
   "metadata": {},
   "source": [
    "# Predicting the class of your image\n",
    "\n",
    "Let's take this bad boy for a spin! Can your image get properly identified?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f32a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 3)\n",
      "(1, 48, 48, 3)\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "(1, 7)\n",
      "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "# Predict the class of your picture.\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    \"./test_folder/maité.png\", target_size=(48, 48)\n",
    ")\n",
    "\n",
    "\n",
    "img_nparray = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "print(img_nparray.shape)\n",
    "# convert image to array\n",
    "\n",
    "x = preprocess_input(img_nparray).reshape((1, 48, 48, 3))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "prediction = new_model.predict(x)\n",
    "\n",
    "print(prediction.shape)\n",
    "\n",
    "# create a list containing the class labels\n",
    "# class_labels = [\"downdog\", \"goddess\", \"plank\", \"tree\", \"warrior2\"]\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "print(class_labels)\n",
    "\n",
    "# find the index of the class with maximum score\n",
    "pred = np.argmax(prediction, axis=-1)\n",
    "class_labels[pred[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
